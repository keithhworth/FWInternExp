{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "#from cleanco import cleanco\n",
    "import re\n",
    "import datetime as dt\n",
    "import os.path\n",
    "import json\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "checked_domains_file = 'CheckedCompanies_20191212.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "after_underscore = checked_domains_file.find('_')+1\n",
    "before_ext = checked_domains_file.find('.csv')\n",
    "checked_file_date = checked_domains_file[after_underscore:before_ext]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'20191212'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checked_file_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 'CheckedCompanies.csv' from industry segmentation Google Sheets document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "checked_domains = pd.read_csv(checked_domains_file)\n",
    "cols = ['Date','Company Name(s)','Email Domain','Industry Segment','Respective Sub-Industry Segment']\n",
    "checked_domains = checked_domains[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1661/1661 [00:01<00:00, 1078.58it/s]\n"
     ]
    }
   ],
   "source": [
    "# Splitting company name(s) attribute and email domains\n",
    "for i in tqdm(range(checked_domains.shape[0])):\n",
    "    co_names = str(checked_domains.iloc[i][1]).lower().split(';')\n",
    "    e_domains = str(checked_domains.iloc[i][2]).lower().split(',')\n",
    "    checked_domains.iloc[i][1] = co_names\n",
    "    checked_domains.iloc[i][2] = e_domains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "checked_domains['Date'] = pd.to_datetime(checked_domains['Date'],errors='coerce').dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "checked_domains.sort_values('Date',ascending=False,inplace=True)\n",
    "checked_domains.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "checked_domains.dropna(subset=['Date'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "checked_array = checked_domains.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Removes whitespace and punctuation in company name and email lists for each company entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1648/1648 [00:00<00:00, 16021.48it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(checked_array.shape[0])):\n",
    "    checked_array[i][0] = checked_array[i][0].strftime('%Y%m%d')\n",
    "    \n",
    "    domain_list = checked_array[i][2]\n",
    "    for d in range(len(domain_list)):\n",
    "        domain_list[d] = domain_list[d].strip()\n",
    "    checked_array[i][2] = domain_list\n",
    "    \n",
    "    company_list = checked_array[i][1]\n",
    "    for c in range(len(company_list)):\n",
    "        #company_list[c] = company_list[c].translate(None, string.punctuation)\n",
    "        company_list[c] = re.sub(r\"[(,.;@#?!&$-)]+\\ *\",\" \", company_list[c])\n",
    "        company_list[c] = ' '.join(company_list[c].split())\n",
    "        #company_list[c] = cleanco(company_list[c].strip()).clean_name()\n",
    "    checked_array[i][1] = company_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['20190703', list(['schneider logistics']),\n",
       "       list(['@schneider.com']), 'Intermediary', '3PL / Brokerage'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checked_array[1500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Checks and creates dictionaries of {'domain':'[Company Info]'} and {'domain':frequency}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary files do not exist, they have been created.\n"
     ]
    }
   ],
   "source": [
    "d_segment_file = 'DictDomainSegments.json'\n",
    "d_duplicate_file = 'DictDomainDuplicates.json'\n",
    "\n",
    "if os.path.exists(d_segment_file) and os.path.exists(d_duplicate_file):\n",
    "    print('Dictionary files already exist.')\n",
    "    with open(d_segment_file,'r+') as dsf, open(d_duplicate_file,'r+') as ddf:\n",
    "        ds = json.load(dsf)\n",
    "        dd = json.load(ddf)\n",
    "        for i in range(checked_array.shape[0]):\n",
    "            for j in range(len(checked_array[i][2])):\n",
    "                domain = checked_array[i][2][j]\n",
    "                if domain not in ds and domain not in dd:\n",
    "                    ds[domain] = [checked_array[i][1][0]\n",
    "                                 ,checked_array[i][3]\n",
    "                                 ,checked_array[i][4]\n",
    "                                 ,checked_array[i][0]\n",
    "                                 ,checked_file_date]\n",
    "                elif domain in ds and domain not in dd:\n",
    "                    if ds.get(domain)[4] < checked_array[i][0] <= checked_file_date: \n",
    "                        temp = {domain:2}\n",
    "                        dd.update(temp)\n",
    "                \n",
    "        dsf.seek(0)\n",
    "        json.dump(ds,dsf)\n",
    "        dsf.truncate()\n",
    "        ddf.seek(0)\n",
    "        json.dump(dd,ddf)\n",
    "        ddf.truncate()\n",
    "else:\n",
    "    print('Dictionary files do not exist, they have been created.')\n",
    "    with open(d_segment_file,'w') as dsf, open(d_duplicate_file,'w') as ddf:\n",
    "        ds = dict()\n",
    "        dd = dict()\n",
    "        for i in range(checked_array.shape[0]):\n",
    "            for j in range(len(checked_array[i][2])):\n",
    "                domain = checked_array[i][2][j]\n",
    "                if domain not in ds:\n",
    "                    ds[domain] = [checked_array[i][1][0]\n",
    "                                ,checked_array[i][3]\n",
    "                                ,checked_array[i][4]\n",
    "                                ,checked_array[i][0]\n",
    "                                ,checked_file_date]\n",
    "                else:\n",
    "                    if domain not in dd:\n",
    "                        #dd[checked_array[i][2][j]] = 2\n",
    "                        temp = {domain:2}\n",
    "                        dd.update(temp)\n",
    "                        \n",
    "                    else:\n",
    "                        #dd[checked_array[i][2][j]] = dd.get(checked_array[i][2][j])+1\n",
    "                        temp = {domain:dd.get(domain)+1}\n",
    "                        dd.update(temp)\n",
    "        json.dump(ds,dsf)\n",
    "        json.dump(dd,ddf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary files do not exist, they have been created.\n"
     ]
    }
   ],
   "source": [
    "cn_segment_file = 'DictCoNameSegments.json'\n",
    "cn_duplicate_file = 'DictCoNameDuplicates.json'\n",
    "\n",
    "if os.path.exists(cn_segment_file) and os.path.exists(cn_duplicate_file):\n",
    "    print('Dictionary files already exist.')\n",
    "    with open(cn_segment_file,'r+') as cnf, open(cn_duplicate_file,'r+') as cndf:\n",
    "        cnd = json.load(cnf)\n",
    "        cndd = json.load(cndf)\n",
    "        for i in range(checked_array.shape[0]):\n",
    "            for j in range(len(checked_array[i][1])):\n",
    "                coname = checked_array[i][1][j]\n",
    "                coname = re.sub(r\"[(,.;@#?!&$-)]+\\ *\",\" \",coname)\n",
    "                coname = ' '.join(coname.split())\n",
    "                if coname not in cnd and coname not in cndd:\n",
    "                    cnd[coname] = [checked_array[i][3]\n",
    "                                 ,checked_array[i][4]\n",
    "                                 ,checked_file_date]\n",
    "                elif coname in cnd and coname not in cndd:\n",
    "                    if cnd.get(coname)[2] < checked_array[i][0] <= checked_file_date: \n",
    "                        temp = {coname:2}\n",
    "                        cndd.update(temp)\n",
    "        cnf.seek(0)\n",
    "        json.dump(cnd,cnf)\n",
    "        cnf.truncate()\n",
    "        cndf.seek(0)\n",
    "        json.dump(cndd,cndf)\n",
    "        cndf.truncate()\n",
    "else:\n",
    "    print('Dictionary files do not exist, they have been created.')\n",
    "    with open(cn_segment_file,'w') as cnf, open(cn_duplicate_file,'w') as cndf:\n",
    "        cnd = dict()\n",
    "        cndd = dict()\n",
    "        for i in range(checked_array.shape[0]):\n",
    "            for j in range(len(checked_array[i][1])):\n",
    "                coname = checked_array[i][1][j]\n",
    "                coname = re.sub(r\"[(,.;@#?!&$-)]+\\ *\",\" \",coname)\n",
    "                coname = ' '.join(coname.split())\n",
    "                if coname not in cnd:\n",
    "                    cnd[coname] = [checked_array[i][3]\n",
    "                                 ,checked_array[i][4]\n",
    "                                 ,checked_file_date]\n",
    "                else:\n",
    "                    if coname not in cndd:\n",
    "                        #dd[checked_array[i][2][j]] = 2\n",
    "                        temp = {coname:2}\n",
    "                        cndd.update(temp)\n",
    "                        \n",
    "                    else:\n",
    "                        #dd[checked_array[i][2][j]] = dd.get(checked_array[i][2][j])+1\n",
    "                        temp = {coname:cndd.get(coname)+1}\n",
    "                        cndd.update(temp)\n",
    "        json.dump(cnd,cnf)\n",
    "        json.dump(cndd,cndf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DictDomainSegments: 2205\n",
      "DictDomainDuplicates: 46\n",
      "DictCoNameSegments: 2229\n",
      "DictCoNameDuplicates: 0\n"
     ]
    }
   ],
   "source": [
    "print('DictDomainSegments: {0}\\nDictDomainDuplicates: {1}\\nDictCoNameSegments: {2}\\nDictCoNameDuplicates: {3}'.format(len(ds.keys()),len(dd.keys()),len(cnd.keys()),len(cndd.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Removes keys in the duplicate dictionary from the segmentation dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd_keys = list(dd.keys())\n",
    "for i in range(len(dd_keys)):\n",
    "    if dd_keys[i] in ds:\n",
    "        del ds[dd_keys[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "cndd_keys = list(cndd.keys())\n",
    "for i in range(len(cndd_keys)):\n",
    "    if cndd_keys[i] in cnd:\n",
    "        del cnd[cndd_keys[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Udpates json files with dictionary changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(d_segment_file,'w') as dsf, open(d_duplicate_file,'w') as ddf, open(cn_segment_file,'w') as cnf, open(cn_duplicate_file,'w') as cndf:\n",
    "    #f.seek(0)\n",
    "    json.dump(ds,dsf)\n",
    "    #f.truncate()\n",
    "    #ddf.seek(0)\n",
    "    json.dump(dd,ddf)\n",
    "    #ddf.truncate()\n",
    "    json.dump(cnd,cnf)\n",
    "    json.dump(cndd,cndf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DictDomainSegments: 2159\n",
      "DictDomainDuplicates: 46\n",
      "DictCoNameSegments: 2229\n",
      "DictCoNameDuplicates: 0\n"
     ]
    }
   ],
   "source": [
    "print('DictDomainSegments: {0}\\nDictDomainDuplicates: {1}\\nDictCoNameSegments: {2}\\nDictCoNameDuplicates: {3}'.format(len(ds.keys()),len(dd.keys()),len(cnd.keys()),len(cndd.keys())))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
